{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines will import required libraries to execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oso/anaconda3/envs/keras-jax/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sae_lens import SAE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment with gemma model 2b and SAEs already trained and provided by gemma scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Disable gradients to avoid running out of memory\n",
    "torch.set_grad_enabled(False)\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2Model(\n",
      "  (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-25): 26 x Gemma2DecoderLayer(\n",
      "      (self_attn): Gemma2Attention(\n",
      "        (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): Gemma2MLP(\n",
      "        (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "        (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "        (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs of layers\n",
    "\n",
    "The following code will execute and compute the cosine similarity across a set of defined pairs of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tensor([     2, 235285,   1938,    577,   1069,   5056,   1355,    577,  14034,\n",
      "           575, 235248, 235284, 235276, 235276, 235276], device='cuda:1')\n",
      "------------------------\n",
      "Comparing layers [0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 0 and 0: 0.12899655103683472\n",
      "------------------------\n",
      "Comparing layers [0, 1]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 0 and 1: 0.025364335626363754\n",
      "------------------------\n",
      "Comparing layers [0, 2]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"I want to time travel back to Spain in 2000\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda:1\")\n",
    "print(len(inputs[0]))\n",
    "print(inputs[0])\n",
    "\n",
    "# Initialize lists to store layer indices and similarities\n",
    "layers = []\n",
    "mean_similarities = []\n",
    "\n",
    "layer_pairs = []\n",
    "# Iterate through layer pairs for comparisons\n",
    "for i in range(15):\n",
    "    layer_pairs.append([0,i])\n",
    "\n",
    "for pair in layer_pairs:\n",
    "    print(f\"------------------------\")\n",
    "    print(f\"Comparing layers {pair}\")\n",
    "    \n",
    "    # Load the SAEs for different layers\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=f\"layer_{str(pair[0])}/width_16k/canonical\")\n",
    "    sae.to(\"cuda:1\")\n",
    "    sae2, cfg_dict2, sparsity2 = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=f\"layer_{str(pair[1])}/width_16k/canonical\")\n",
    "    sae2.to(\"cuda:1\")\n",
    "\n",
    "    # Function to gather residual activations from a specified layer\n",
    "    def gather_activations(model, target_layer, inputs):\n",
    "        target_act = None\n",
    "        \n",
    "        def hook(mod, inputs, outputs):\n",
    "            nonlocal target_act\n",
    "            target_act = outputs[0]\n",
    "            return outputs\n",
    "\n",
    "        handle = model.model.layers[target_layer].register_forward_hook(hook)\n",
    "        _ = model.forward(inputs)\n",
    "        handle.remove()\n",
    "        return target_act\n",
    "\n",
    "    # Gather residual activations for layers\n",
    "    act_layer_base = gather_activations(model, pair[0], inputs)\n",
    "    act_layer_compare = gather_activations(model, pair[1], inputs)\n",
    "\n",
    "    # Encode the activations using the corresponding SAEs\n",
    "    sae_acts_base = sae.encode(act_layer_base.to(torch.float32))\n",
    "    sae_acts_compare = sae2.encode(act_layer_compare.to(torch.float32))\n",
    "    print(sae_acts_base.shape)\n",
    "\n",
    "\n",
    "    # Reshape activations for pairwise comparison\n",
    "    sae_acts_base_flat = sae_acts_base.squeeze(0).reshape(-1, 16384)\n",
    "    sae_acts_compare_flat = sae_acts_compare.squeeze(0).reshape(-1, 16384)\n",
    "    print(len(sae_acts_base_flat))\n",
    "    print(len(sae_acts_compare_flat))\n",
    "    \n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = cosine_similarity(sae_acts_base_flat.cpu().numpy(), sae_acts_compare_flat.cpu().numpy())\n",
    "\n",
    "    # Normalize cosine similarity values to the range [0, 1]\n",
    "    cosine_sim = (cosine_sim - cosine_sim.min()) / (cosine_sim.max() - cosine_sim.min())\n",
    "\n",
    "    # Compute mean similarity\n",
    "    mean_similarity = cosine_sim.mean()\n",
    "    print(f\"Mean Cosine Similarity Between Layers {pair[0]} and {pair[1]}: {mean_similarity}\")\n",
    "\n",
    "    # Store the layer and mean similarity\n",
    "    layers.append(pair[1])\n",
    "    mean_similarities.append(mean_similarity)\n",
    "\n",
    "    # Plot the cosine similarity heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_sim, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=True)\n",
    "    plt.title(f\"Cosine Similarity Heatmap Between Layer {pair[0]} and Layer {pair[1]}\")\n",
    "    plt.xlabel(f\"Layer {pair[0]} Tokens\")\n",
    "    plt.ylabel(f\"Layer {pair[1]} Tokens\")\n",
    "    filename = f\"./output/token/cosine_similarity_heatmap_layers_{pair[0]}_and_{pair[1]}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    del sae\n",
    "    del sae2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Plot the mean cosine similarity trend across layers\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "min_value = min(mean_similarities)\n",
    "max_value = max(mean_similarities)\n",
    "\n",
    "# Normalize each value in the list\n",
    "mean_similarities = [(value - min_value) / (max_value - min_value) for value in mean_similarities]\n",
    "plt.plot(layers, mean_similarities, marker='o')\n",
    "plt.title(\"Mean Cosine Similarity Between Layer 0 and Other Layers\")\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Mean Normalized Cosine Similarity\")\n",
    "plt.grid()\n",
    "filename = \"./output/mean_cosine_similarity_trend.png\"\n",
    "plt.savefig(filename)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "tensor([     2, 235285,   1938,    577,   1069,   5056,   1355,    577,  14034,\n",
      "           575, 235248, 235284, 235276, 235276, 235276], device='cuda:1')\n",
      "------------------------\n",
      "Comparing layers [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 0 and 1: 0.025364335626363754\n",
      "------------------------\n",
      "Comparing layers [1, 2]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 1 and 2: 0.010482142679393291\n",
      "------------------------\n",
      "Comparing layers [2, 3]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 2 and 3: 0.02757006697356701\n",
      "------------------------\n",
      "Comparing layers [3, 4]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 3 and 4: 0.022210560739040375\n",
      "------------------------\n",
      "Comparing layers [4, 5]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 4 and 5: 0.02662784792482853\n",
      "------------------------\n",
      "Comparing layers [5, 6]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 5 and 6: 0.0184357687830925\n",
      "------------------------\n",
      "Comparing layers [6, 7]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 6 and 7: 0.0148434117436409\n",
      "------------------------\n",
      "Comparing layers [7, 8]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 7 and 8: 0.03247479721903801\n",
      "------------------------\n",
      "Comparing layers [8, 9]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 8 and 9: 0.03662450984120369\n",
      "------------------------\n",
      "Comparing layers [9, 10]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 9 and 10: 0.03724077716469765\n",
      "------------------------\n",
      "Comparing layers [10, 11]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 10 and 11: 0.03440893068909645\n",
      "------------------------\n",
      "Comparing layers [11, 12]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 11 and 12: 0.07840654253959656\n",
      "------------------------\n",
      "Comparing layers [12, 13]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 12 and 13: 0.1305893510580063\n",
      "------------------------\n",
      "Comparing layers [13, 14]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 13 and 14: 0.0737876147031784\n",
      "------------------------\n",
      "Comparing layers [14, 15]\n",
      "torch.Size([1, 15, 16384])\n",
      "15\n",
      "15\n",
      "Mean Cosine Similarity Between Layers 14 and 15: 0.018691156059503555\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"I want to time travel back to Spain in 2000\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda:1\")\n",
    "print(len(inputs[0]))\n",
    "print(inputs[0])\n",
    "\n",
    "# Initialize lists to store layer indices and similarities\n",
    "layers = []\n",
    "mean_similarities = []\n",
    "\n",
    "layer_pairs = []\n",
    "# Iterate through layer pairs for comparisons\n",
    "for i in range(15):\n",
    "    layer_pairs.append([i,i+1])\n",
    "\n",
    "for pair in layer_pairs:\n",
    "    print(f\"------------------------\")\n",
    "    print(f\"Comparing layers {pair}\")\n",
    "    \n",
    "    # Load the SAEs for different layers\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=f\"layer_{str(pair[0])}/width_16k/canonical\")\n",
    "    sae.to(\"cuda:1\")\n",
    "    sae2, cfg_dict2, sparsity2 = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=f\"layer_{str(pair[1])}/width_16k/canonical\")\n",
    "    sae2.to(\"cuda:1\")\n",
    "\n",
    "    # Function to gather residual activations from a specified layer\n",
    "    def gather_activations(model, target_layer, inputs):\n",
    "        target_act = None\n",
    "        \n",
    "        def hook(mod, inputs, outputs):\n",
    "            nonlocal target_act\n",
    "            target_act = outputs[0]\n",
    "            return outputs\n",
    "\n",
    "        handle = model.model.layers[target_layer].register_forward_hook(hook)\n",
    "        _ = model.forward(inputs)\n",
    "        handle.remove()\n",
    "        return target_act\n",
    "\n",
    "    # Gather residual activations for layers\n",
    "    act_layer_base = gather_activations(model, pair[0], inputs)\n",
    "    act_layer_compare = gather_activations(model, pair[1], inputs)\n",
    "\n",
    "    # Encode the activations using the corresponding SAEs\n",
    "    sae_acts_base = sae.encode(act_layer_base.to(torch.float32))\n",
    "    sae_acts_compare = sae2.encode(act_layer_compare.to(torch.float32))\n",
    "    print(sae_acts_base.shape)\n",
    "\n",
    "\n",
    "    # Reshape activations for pairwise comparison\n",
    "    sae_acts_base_flat = sae_acts_base.squeeze(0).reshape(-1, 16384)\n",
    "    sae_acts_compare_flat = sae_acts_compare.squeeze(0).reshape(-1, 16384)\n",
    "    print(len(sae_acts_base_flat))\n",
    "    print(len(sae_acts_compare_flat))\n",
    "    \n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = cosine_similarity(sae_acts_base_flat.cpu().numpy(), sae_acts_compare_flat.cpu().numpy())\n",
    "\n",
    "    # Normalize cosine similarity values to the range [0, 1]\n",
    "    cosine_sim = (cosine_sim - cosine_sim.min()) / (cosine_sim.max() - cosine_sim.min())\n",
    "\n",
    "    # Compute mean similarity\n",
    "    mean_similarity = cosine_sim.mean()\n",
    "    print(f\"Mean Cosine Similarity Between Layers {pair[0]} and {pair[1]}: {mean_similarity}\")\n",
    "\n",
    "    # Store the layer and mean similarity\n",
    "    layers.append(pair[1])\n",
    "    mean_similarities.append(mean_similarity)\n",
    "\n",
    "    # Plot the cosine similarity heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_sim, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=True)\n",
    "    plt.title(f\"Cosine Similarity Heatmap Between Layer {pair[0]} and Layer {pair[1]}\")\n",
    "    plt.xlabel(f\"Layer {pair[0]} Tokens\")\n",
    "    plt.ylabel(f\"Layer {pair[1]} Tokens\")\n",
    "    filename = f\"./output/with_prev/cosine_similarity_heatmap_layers_{pair[0]}_and_{pair[1]}.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    del sae\n",
    "    del sae2\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Plot the mean cosine similarity trend across layers\n",
    "min_value = min(mean_similarities)\n",
    "max_value = max(mean_similarities)\n",
    "\n",
    "# Normalize each value in the list\n",
    "mean_similarities = [(value - min_value) / (max_value - min_value) for value in mean_similarities]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(layers, mean_similarities, marker='o')\n",
    "plt.title(\"Mean Cosine Similarity Between Layer prev and next Layers\")\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Mean Normalized Cosine Similarity\")\n",
    "plt.grid()\n",
    "filename = \"./output/mean_cosine_similarity_trend_with_previous.png\"\n",
    "plt.savefig(filename)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
